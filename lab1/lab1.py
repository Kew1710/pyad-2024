# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YqvNxO6UORnvj7t0ClFF-NnmQSJ5kkFk
"""

!gdown 1NQuold7jvGZfHikbvtj0QynbhM90pT8L
!gdown 1HNAGhPHE6BmamcI0D767-BFhF4BWs6Hh
!gdown 1ERgltJRuybLl7sIUUVckzpUVzyP1ttCq
!gdown 1XrUk6ZYwQLQTjtsMHKxzzERP4dCLMEao
!gdown 1qy95Qpd93VClYPethMtMDJdAMBl21fnW
!gdown 1js5pbc_xe6kP7W4_lp6HaeXR0oAw2ZDL
!gdown 1qb4_uKGy0KU_4jRzaauqeObaaCQMtdBZ
!gdown 1TuC71TgrJDgpOvQVKl9jn89fAr9El1Pa
!gdown 1nt7e9Pszf2-IB3GsKHNRHZ_SPzxS0E2L
!gdown 1V5tCiD9jRCss0KoJxk-r1zOCw3onwez3
!gdown 1WsKGiVBV2p-sgiDePT6HZKUezWPc2zUH

!sudo apt update
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.5.tgz
!wget -q https://dlcdn.apache.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.5.tgz
!tar xf spark-3.5.3-bin-hadoop3.5.tgz
!pip install -q findspark
!pip install pyspark
!pip install py4j

import os
import sys


import findspark
findspark.init()
findspark.find()

import pyspark

from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.functions import col, month, sum, when, desc, greatest, count
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, month, sum, when, desc, greatest, count, to_date
from typing import List
import pyspark.sql.types as T
import pyspark.sql.functions as F

spark= SparkSession \
       .builder \
       .appName("Our First Spark Example") \
       .getOrCreate()

df = spark.read.csv('/content/T_ONTIME_REPORTING.csv', header=True, inferSchema=True)
df = df.union(spark.read.csv('/content/T_ONTIME_REPORTING 2.csv', header=True, inferSchema=True))
df = df.union(spark.read.csv('/content/T_ONTIME_REPORTING 3.csv', header=True, inferSchema=True))
df = df.union(spark.read.csv('/content/T_ONTIME_REPORTING 4.csv', header=True, inferSchema=True))
df = df.union(spark.read.csv('/content/T_ONTIME_REPORTING 5.csv', header=True, inferSchema=True))
df = df.union(spark.read.csv('/content/T_ONTIME_REPORTING 6.csv', header=True, inferSchema=True))
df = df.union(spark.read.csv('/content/T_ONTIME_REPORTING 7.csv', header=True, inferSchema=True))
df = df.union(spark.read.csv('/content/T_ONTIME_REPORTING 8.csv', header=True, inferSchema=True))
df = df.union(spark.read.csv('/content/T_ONTIME_REPORTING 9.csv', header=True, inferSchema=True))
df = df.union(spark.read.csv('/content/T_ONTIME_REPORTING 10.csv', header=True, inferSchema=True))

unique_carrier = df.select("OP_UNIQUE_CARRIER").distinct()
print(unique_carrier.show())

top5_carrier = (df.groupBy('OP_UNIQUE_CARRIER')
.count().orderBy("count", ascending=False).limit(5))
print(top5_carrier.show())

#Направление - это город прилета
top10_direct = (df.groupBy('DEST_CITY_NAME')
.count().orderBy("count", ascending=False).limit(10))
print(top10_direct.show())

#Направление - это пара «аэропорт вылета-аэропорт прилета»
top10_direct = (
    df.groupBy("ORIGIN_AIRPORT_ID", "DEST_AIRPORT_ID")
    .count()
    .orderBy("count", ascending=False)
    .limit(10)  # Оставляем только одну самую частую пару
)
print(top10_direct.show())

#Направление - это пара «город вылета-город прилета»
top10_direct = (
    df.groupBy("ORIGIN_CITY_NAME", "DEST_CITY_NAME")
    .count()
    .orderBy("count", ascending=False)
    .limit(10)  # Оставляем только одну самую частую пару
)
print(top10_direct.show())



from pyspark.sql.functions import substring_index, substring
from pyspark.sql.functions import col, sum, desc, when, count, max, lit
from pyspark.sql import functions as F
df_month = df.withColumn('MONTH', substring_index(col("FL_DATE"), '/', 1))
cancelled_stats = (
    df_month.groupBy("MONTH")
    .agg(sum(col("CANCELLED")).alias("CANCELLED_COUNT"))
    .orderBy(desc("CANCELLED_COUNT"))

)

delay_reasons = [
    "CARRIER_DELAY", "WEATHER_DELAY", "NAS_DELAY", "SECURITY_DELAY", "LATE_AIRCRAFT_DELAY"
]

# Создание новых столбцов с подсчетом причин задержек
for reason in delay_reasons:
    df_month = df_month.withColumn(
        f"{reason}_count",
        when(col(reason).isNotNull(), 1).otherwise(0)
    )

# Агрегация данных, чтобы посчитать количество причин задержек для каждого месяца
reason_counts = (
    df_month.groupBy("MONTH")
    .agg(
        *[sum(f"{reason}_count").alias(f"{reason}_count") for reason in delay_reasons]
    )
)

# Определяем основную причину задержки для каждого месяца
reason_counts = reason_counts.withColumn(
    "MAX_DELAY_REASON",
    F.when(col("CARRIER_DELAY_count") == F.greatest(*[col(f"{reason}_count") for reason in delay_reasons]), lit("CARRIER_DELAY"))
    .when(col("WEATHER_DELAY_count") == F.greatest(*[col(f"{reason}_count") for reason in delay_reasons]), lit("WEATHER_DELAY"))
    .when(col("NAS_DELAY_count") == F.greatest(*[col(f"{reason}_count") for reason in delay_reasons]), lit("NAS_DELAY"))
    .when(col("SECURITY_DELAY_count") == F.greatest(*[col(f"{reason}_count") for reason in delay_reasons]), lit("SECURITY_DELAY"))
    .when(col("LATE_AIRCRAFT_DELAY_count") == F.greatest(*[col(f"{reason}_count") for reason in delay_reasons]), lit("LATE_AIRCRAFT_DELAY"))
)

# Выводим результаты
cancelled_stats.show()
reason_counts.show()

from pyspark.ml.feature import StringIndexer, OneHotEncoder

carr_indexer = StringIndexer(inputCol="OP_UNIQUE_CARRIER", outputCol="carrier_index")
carr_encoder = OneHotEncoder(inputCol="carrier_index", outputCol="carrier_fact")

origin_indexer = StringIndexer(inputCol="ORIGIN_CITY_NAME", outputCol="origin_index")
origin_encoder = OneHotEncoder(inputCol="origin_index", outputCol="origin_fact")

dest_indexer = StringIndexer(inputCol="DEST_CITY_NAME", outputCol="dest_index")
dest_encoder = OneHotEncoder(inputCol="dest_index", outputCol="dest_fact")

"""Выделите целевую переменную. Ее важно назвать label. По этому названию дальше метод fit() отличит ее от других столбцов. Целевая переменная должна получиться из значений столбца CANCELLED (отмена рейса)."""

df3 = df.withColumn("label", col("CANCELLED").cast("integer"))

"""Соберите все признаки, которые будут использоваться для обучения, в один вектор features. В столбце DATE должен лежать месяц, в который был совершен рейс."""

from pyspark.ml.feature import VectorAssembler

vec_assembler = VectorAssembler(inputCols=["DATE", "DISTANCE", "carrier_fact", "dest_fact", "origin_fact"], outputCol="features")
df3 = df3.withColumn('DATE', substring_index(col("FL_DATE"), '/', 1).cast("int"))

"""Объедините все предобработки в один пайплайн. Он будет состоять из кодирования категориальных переменных и объединения новых столбцов с некоторыми старыми в матрицу признаков."""

from pyspark.ml import Pipeline

flights_pipe = Pipeline(stages=[dest_indexer, dest_encoder, carr_indexer, carr_encoder, origin_indexer, origin_encoder, vec_assembler])

piped_data = flights_pipe.fit(df3).transform(df3)

training, test = piped_data.randomSplit([.7, .3])

from pyspark.ml.classification import LogisticRegression

lr = LogisticRegression()

model = lr.fit(training)
print(model)

from pyspark.ml.evaluation import BinaryClassificationEvaluator

test_results = model.transform(test)

evaluator = BinaryClassificationEvaluator(rawPredictionCol="rawPrediction", labelCol="label", metricName="areaUnderROC")

print(evaluator.evaluate(test_results))

def evaluate_model(training_data, test_data, features_to_drop=None):
    """Evaluates a logistic regression model with specified features dropped.

    Args:
        training_data: Spark DataFrame for training.
        test_data: Spark DataFrame for testing.
        features_to_drop: List of feature names to exclude.

    Returns:
        The area under ROC score of the model.
    """

    input_cols = ["DATE", "DISTANCE", "carrier_fact", "dest_fact", "origin_fact"]
    if features_to_drop:
      input_cols = [col for col in input_cols if col.split("_")[0] not in features_to_drop]

    vec_assembler = VectorAssembler(inputCols=input_cols, outputCol="features")

    flights_pipe = Pipeline(stages=[vec_assembler])

    training_data = training_data.drop("features")
    piped_data = flights_pipe.fit(training_data).transform(training_data)

    training, test = piped_data.randomSplit([.7, .3])

    lr = LogisticRegression()
    model = lr.fit(training)

    test_results = model.transform(test)
    evaluator = BinaryClassificationEvaluator(rawPredictionCol="rawPrediction", labelCol="label", metricName="areaUnderROC")
    return evaluator.evaluate(test_results)


# Original features
original_score = evaluate_model(training, test)
print(f"Original model score: {original_score}")

# Without origin
no_origin_score = evaluate_model(training, test, features_to_drop=["origin"])
print(f"Model without origin score: {no_origin_score}")

# Without destination
no_dest_score = evaluate_model(training, test, features_to_drop=["dest"])
print(f"Model without destination score: {no_dest_score}")

# Without carrier
no_carrier_score = evaluate_model(training, test, features_to_drop=["carrier"])
print(f"Model without carrier score: {no_carrier_score}")

# Without month
no_month_score = evaluate_model(training, test, features_to_drop=["DATE"])
print(f"Model without month score: {no_month_score}")