{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPHQiUE5zFWn",
        "outputId": "f4d5026b-45e3-4674-c2df-e7b03e0b449e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1NQuold7jvGZfHikbvtj0QynbhM90pT8L\n",
            "To: /content/T_ONTIME_REPORTING.csv\n",
            "100% 67.6M/67.6M [00:01<00:00, 67.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1HNAGhPHE6BmamcI0D767-BFhF4BWs6Hh\n",
            "To: /content/T_ONTIME_REPORTING 2.csv\n",
            "100% 66.4M/66.4M [00:00<00:00, 121MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ERgltJRuybLl7sIUUVckzpUVzyP1ttCq\n",
            "To: /content/T_ONTIME_REPORTING 3.csv\n",
            "100% 61.6M/61.6M [00:00<00:00, 70.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1XrUk6ZYwQLQTjtsMHKxzzERP4dCLMEao\n",
            "To: /content/T_ONTIME_REPORTING 4.csv\n",
            "100% 71.8M/71.8M [00:00<00:00, 149MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qy95Qpd93VClYPethMtMDJdAMBl21fnW\n",
            "To: /content/T_ONTIME_REPORTING 5.csv\n",
            "100% 69.3M/69.3M [00:00<00:00, 133MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1js5pbc_xe6kP7W4_lp6HaeXR0oAw2ZDL\n",
            "To: /content/T_ONTIME_REPORTING 6.csv\n",
            "100% 71.1M/71.1M [00:01<00:00, 45.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qb4_uKGy0KU_4jRzaauqeObaaCQMtdBZ\n",
            "To: /content/T_ONTIME_REPORTING 7.csv\n",
            "100% 71.7M/71.7M [00:00<00:00, 129MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1TuC71TgrJDgpOvQVKl9jn89fAr9El1Pa\n",
            "To: /content/T_ONTIME_REPORTING 8.csv\n",
            "100% 74.9M/74.9M [00:01<00:00, 39.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1nt7e9Pszf2-IB3GsKHNRHZ_SPzxS0E2L\n",
            "To: /content/T_ONTIME_REPORTING 9.csv\n",
            "100% 74.3M/74.3M [00:00<00:00, 106MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1V5tCiD9jRCss0KoJxk-r1zOCw3onwez3\n",
            "To: /content/T_ONTIME_REPORTING 10.csv\n",
            "100% 69.3M/69.3M [00:00<00:00, 150MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1WsKGiVBV2p-sgiDePT6HZKUezWPc2zUH\n",
            "To: /content/T_ONTIME_REPORTING 11.csv\n",
            "100% 70.3M/70.3M [00:00<00:00, 118MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1NQuold7jvGZfHikbvtj0QynbhM90pT8L\n",
        "!gdown 1HNAGhPHE6BmamcI0D767-BFhF4BWs6Hh\n",
        "!gdown 1ERgltJRuybLl7sIUUVckzpUVzyP1ttCq\n",
        "!gdown 1XrUk6ZYwQLQTjtsMHKxzzERP4dCLMEao\n",
        "!gdown 1qy95Qpd93VClYPethMtMDJdAMBl21fnW\n",
        "!gdown 1js5pbc_xe6kP7W4_lp6HaeXR0oAw2ZDL\n",
        "!gdown 1qb4_uKGy0KU_4jRzaauqeObaaCQMtdBZ\n",
        "!gdown 1TuC71TgrJDgpOvQVKl9jn89fAr9El1Pa\n",
        "!gdown 1nt7e9Pszf2-IB3GsKHNRHZ_SPzxS0E2L\n",
        "!gdown 1V5tCiD9jRCss0KoJxk-r1zOCw3onwez3\n",
        "!gdown 1WsKGiVBV2p-sgiDePT6HZKUezWPc2zUH\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ymqy201Yzbf3",
        "outputId": "b803d29b-6039-4f23-d369-5888a7e82d58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:7 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [62.7 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,285 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,623 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,520 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,861 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,646 kB]\n",
            "Fetched 17.4 MB in 3s (5,330 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "52 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.5.3-bin-hadoop3.5.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.5.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.5.tgz\n",
        "!tar xf spark-3.5.3-bin-hadoop3.5.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.functions import col, month, sum, when, desc, greatest, count\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, month, sum, when, desc, greatest, count, to_date\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGHD2teKzc-t"
      },
      "outputs": [],
      "source": [
        "df = spark.read.csv('/content/T_ONTIME_REPORTING.csv', header=True, inferSchema=True)\n",
        "df = df.union(spark.read.csv('/content/T_ONTIME_REPORTING 2.csv', header=True, inferSchema=True))\n",
        "df = df.union(spark.read.csv('/content/T_ONTIME_REPORTING 3.csv', header=True, inferSchema=True))\n",
        "df = df.union(spark.read.csv('/content/T_ONTIME_REPORTING 4.csv', header=True, inferSchema=True))\n",
        "df = df.union(spark.read.csv('/content/T_ONTIME_REPORTING 5.csv', header=True, inferSchema=True))\n",
        "df = df.union(spark.read.csv('/content/T_ONTIME_REPORTING 6.csv', header=True, inferSchema=True))\n",
        "df = df.union(spark.read.csv('/content/T_ONTIME_REPORTING 7.csv', header=True, inferSchema=True))\n",
        "df = df.union(spark.read.csv('/content/T_ONTIME_REPORTING 8.csv', header=True, inferSchema=True))\n",
        "df = df.union(spark.read.csv('/content/T_ONTIME_REPORTING 9.csv', header=True, inferSchema=True))\n",
        "df = df.union(spark.read.csv('/content/T_ONTIME_REPORTING 10.csv', header=True, inferSchema=True))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewC4YfQSzfT3",
        "outputId": "e59e14c5-fdc8-4375-c490-476d731c8249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|OP_UNIQUE_CARRIER|\n",
            "+-----------------+\n",
            "|               UA|\n",
            "|               NK|\n",
            "|               AA|\n",
            "|               B6|\n",
            "|               DL|\n",
            "|               OO|\n",
            "|               F9|\n",
            "|               MQ|\n",
            "|               OH|\n",
            "|               HA|\n",
            "|               G4|\n",
            "|               YX|\n",
            "|               AS|\n",
            "|               WN|\n",
            "|               9E|\n",
            "+-----------------+\n",
            "\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "unique_carrier = df.select(\"OP_UNIQUE_CARRIER\").distinct()\n",
        "print(unique_carrier.show())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTkOWHjg19Mv",
        "outputId": "bdceae41-6b29-4884-8e50-697f8836b53f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------+\n",
            "|OP_UNIQUE_CARRIER|  count|\n",
            "+-----------------+-------+\n",
            "|               WN|1179623|\n",
            "|               DL| 811280|\n",
            "|               AA| 784153|\n",
            "|               UA| 602736|\n",
            "|               OO| 556873|\n",
            "+-----------------+-------+\n",
            "\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "top5_carrier = (df.groupBy('OP_UNIQUE_CARRIER')\n",
        ".count().orderBy(\"count\", ascending=False).limit(5))\n",
        "print(top5_carrier.show())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aHLfMee269Z",
        "outputId": "154c870f-8f8e-4f50-ca1b-4ca12d599387"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+\n",
            "|      DEST_CITY_NAME| count|\n",
            "+--------------------+------+\n",
            "|         Chicago, IL|282723|\n",
            "|         Atlanta, GA|277089|\n",
            "|        New York, NY|247223|\n",
            "|          Denver, CO|237001|\n",
            "|Dallas/Fort Worth...|234809|\n",
            "|       Charlotte, NC|161726|\n",
            "|     Los Angeles, CA|160652|\n",
            "|      Washington, DC|157883|\n",
            "|       Las Vegas, NV|156322|\n",
            "|         Phoenix, AZ|150906|\n",
            "+--------------------+------+\n",
            "\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "#Направление - это город прилета\n",
        "top10_direct = (df.groupBy('DEST_CITY_NAME')\n",
        ".count().orderBy(\"count\", ascending=False).limit(10))\n",
        "print(top10_direct.show())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsScZHIW29Ux",
        "outputId": "b1e6a051-2ed0-4e98-b91e-b2fa954cacd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+---------------+-----+\n",
            "|ORIGIN_AIRPORT_ID|DEST_AIRPORT_ID|count|\n",
            "+-----------------+---------------+-----+\n",
            "|            13830|          12173|10504|\n",
            "|            12173|          13830|10487|\n",
            "|            14771|          12892| 9631|\n",
            "|            12892|          14771| 9631|\n",
            "|            12892|          12889| 8739|\n",
            "|            12889|          12892| 8715|\n",
            "|            12953|          13930| 8432|\n",
            "|            13930|          12953| 8432|\n",
            "|            11278|          10721| 8306|\n",
            "|            10721|          11278| 8305|\n",
            "+-----------------+---------------+-----+\n",
            "\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "#Направление - это пара «аэропорт вылета-аэропорт прилета»\n",
        "top10_direct = (\n",
        "    df.groupBy(\"ORIGIN_AIRPORT_ID\", \"DEST_AIRPORT_ID\")\n",
        "    .count()\n",
        "    .orderBy(\"count\", ascending=False)\n",
        "    .limit(10)  # Оставляем только одну самую частую пару\n",
        ")\n",
        "print(top10_direct.show())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gcr1DM8N4da9",
        "outputId": "2e4052f6-3bbb-4589-89b1-bb6accd85604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----------------+-----+\n",
            "| ORIGIN_CITY_NAME|   DEST_CITY_NAME|count|\n",
            "+-----------------+-----------------+-----+\n",
            "|      Chicago, IL|     New York, NY|12856|\n",
            "|     New York, NY|      Chicago, IL|12855|\n",
            "|       Boston, MA|     New York, NY|12337|\n",
            "|     New York, NY|       Boston, MA|12318|\n",
            "|      Kahului, HI|     Honolulu, HI|10504|\n",
            "|     Honolulu, HI|      Kahului, HI|10487|\n",
            "|        Miami, FL|     New York, NY| 9711|\n",
            "|     New York, NY|        Miami, FL| 9709|\n",
            "|  Los Angeles, CA|San Francisco, CA| 9631|\n",
            "|San Francisco, CA|  Los Angeles, CA| 9631|\n",
            "+-----------------+-----------------+-----+\n",
            "\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "#Направление - это пара «город вылета-город прилета»\n",
        "top10_direct = (\n",
        "    df.groupBy(\"ORIGIN_CITY_NAME\", \"DEST_CITY_NAME\")\n",
        "    .count()\n",
        "    .orderBy(\"count\", ascending=False)\n",
        "    .limit(10)  # Оставляем только одну самую частую пару\n",
        ")\n",
        "print(top10_direct.show())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3bqDtUVct1P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl2LPv4Y4tze",
        "outputId": "97bc1c24-ae44-4cfd-a1ff-bfa7ddd26765"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------------+\n",
            "|MONTH|CANCELLED_COUNT|\n",
            "+-----+---------------+\n",
            "|    1|        30684.0|\n",
            "|    7|        14606.0|\n",
            "|    6|        12219.0|\n",
            "|    4|         9589.0|\n",
            "|    8|         9172.0|\n",
            "|    2|         9019.0|\n",
            "|    3|         7406.0|\n",
            "|    5|         3310.0|\n",
            "|   11|          819.0|\n",
            "+-----+---------------+\n",
            "\n",
            "+-----+-------------------+-------------------+---------------+--------------------+-------------------------+----------------+\n",
            "|MONTH|CARRIER_DELAY_count|WEATHER_DELAY_count|NAS_DELAY_count|SECURITY_DELAY_count|LATE_AIRCRAFT_DELAY_count|MAX_DELAY_REASON|\n",
            "+-----+-------------------+-------------------+---------------+--------------------+-------------------------+----------------+\n",
            "|    1|             243123|             243123|         243123|              243123|                   243123|   CARRIER_DELAY|\n",
            "|    2|              92969|              92969|          92969|               92969|                    92969|   CARRIER_DELAY|\n",
            "|    3|             136044|             136044|         136044|              136044|                   136044|   CARRIER_DELAY|\n",
            "|    4|             125882|             125882|         125882|              125882|                   125882|   CARRIER_DELAY|\n",
            "|    5|             104119|             104119|         104119|              104119|                   104119|   CARRIER_DELAY|\n",
            "|    6|             152268|             152268|         152268|              152268|                   152268|   CARRIER_DELAY|\n",
            "|    7|             167080|             167080|         167080|              167080|                   167080|   CARRIER_DELAY|\n",
            "|    8|             128439|             128439|         128439|              128439|                   128439|   CARRIER_DELAY|\n",
            "|   11|              76347|              76347|          76347|               76347|                    76347|   CARRIER_DELAY|\n",
            "+-----+-------------------+-------------------+---------------+--------------------+-------------------------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import substring_index, substring\n",
        "from pyspark.sql.functions import col, sum, desc, when, count, max, lit\n",
        "from pyspark.sql import functions as F\n",
        "df_month = df.withColumn('MONTH', substring_index(col(\"FL_DATE\"), '/', 1))\n",
        "cancelled_stats = (\n",
        "    df_month.groupBy(\"MONTH\")\n",
        "    .agg(sum(col(\"CANCELLED\")).alias(\"CANCELLED_COUNT\"))\n",
        "    .orderBy(desc(\"CANCELLED_COUNT\"))\n",
        "\n",
        ")\n",
        "\n",
        "delay_reasons = [\n",
        "    \"CARRIER_DELAY\", \"WEATHER_DELAY\", \"NAS_DELAY\", \"SECURITY_DELAY\", \"LATE_AIRCRAFT_DELAY\"\n",
        "]\n",
        "\n",
        "# Создание новых столбцов с подсчетом причин задержек\n",
        "for reason in delay_reasons:\n",
        "    df_month = df_month.withColumn(\n",
        "        f\"{reason}_count\",\n",
        "        when(col(reason).isNotNull(), 1).otherwise(0)\n",
        "    )\n",
        "\n",
        "# Агрегация данных, чтобы посчитать количество причин задержек для каждого месяца\n",
        "reason_counts = (\n",
        "    df_month.groupBy(\"MONTH\")\n",
        "    .agg(\n",
        "        *[sum(f\"{reason}_count\").alias(f\"{reason}_count\") for reason in delay_reasons]\n",
        "    )\n",
        ")\n",
        "\n",
        "# Определяем основную причину задержки для каждого месяца\n",
        "reason_counts = reason_counts.withColumn(\n",
        "    \"MAX_DELAY_REASON\",\n",
        "    F.when(col(\"CARRIER_DELAY_count\") == F.greatest(*[col(f\"{reason}_count\") for reason in delay_reasons]), lit(\"CARRIER_DELAY\"))\n",
        "    .when(col(\"WEATHER_DELAY_count\") == F.greatest(*[col(f\"{reason}_count\") for reason in delay_reasons]), lit(\"WEATHER_DELAY\"))\n",
        "    .when(col(\"NAS_DELAY_count\") == F.greatest(*[col(f\"{reason}_count\") for reason in delay_reasons]), lit(\"NAS_DELAY\"))\n",
        "    .when(col(\"SECURITY_DELAY_count\") == F.greatest(*[col(f\"{reason}_count\") for reason in delay_reasons]), lit(\"SECURITY_DELAY\"))\n",
        "    .when(col(\"LATE_AIRCRAFT_DELAY_count\") == F.greatest(*[col(f\"{reason}_count\") for reason in delay_reasons]), lit(\"LATE_AIRCRAFT_DELAY\"))\n",
        ")\n",
        "\n",
        "# Выводим результаты\n",
        "cancelled_stats.show()\n",
        "reason_counts.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WmYvRUUkmsf"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
        "\n",
        "carr_indexer = StringIndexer(inputCol=\"OP_UNIQUE_CARRIER\", outputCol=\"carrier_index\")\n",
        "carr_encoder = OneHotEncoder(inputCol=\"carrier_index\", outputCol=\"carrier_fact\")\n",
        "\n",
        "origin_indexer = StringIndexer(inputCol=\"ORIGIN_CITY_NAME\", outputCol=\"origin_index\")\n",
        "origin_encoder = OneHotEncoder(inputCol=\"origin_index\", outputCol=\"origin_fact\")\n",
        "\n",
        "dest_indexer = StringIndexer(inputCol=\"DEST_CITY_NAME\", outputCol=\"dest_index\")\n",
        "dest_encoder = OneHotEncoder(inputCol=\"dest_index\", outputCol=\"dest_fact\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выделите целевую переменную. Ее важно назвать label. По этому названию дальше метод fit() отличит ее от других столбцов. Целевая переменная должна получиться из значений столбца CANCELLED (отмена рейса)."
      ],
      "metadata": {
        "id": "ISHdz4JfgRtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = df.withColumn(\"label\", col(\"CANCELLED\").cast(\"integer\"))"
      ],
      "metadata": {
        "id": "tdpStLLefiC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Соберите все признаки, которые будут использоваться для обучения, в один вектор features. В столбце DATE должен лежать месяц, в который был совершен рейс."
      ],
      "metadata": {
        "id": "Czt5RQ0CfhxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "vec_assembler = VectorAssembler(inputCols=[\"DATE\", \"DISTANCE\", \"carrier_fact\", \"dest_fact\", \"origin_fact\"], outputCol=\"features\")\n",
        "df3 = df3.withColumn('DATE', substring_index(col(\"FL_DATE\"), '/', 1).cast(\"int\"))"
      ],
      "metadata": {
        "id": "2M3wAvm0gZp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объедините все предобработки в один пайплайн. Он будет состоять из кодирования категориальных переменных и объединения новых столбцов с некоторыми старыми в матрицу признаков."
      ],
      "metadata": {
        "id": "RQ5ikPaOge1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "flights_pipe = Pipeline(stages=[dest_indexer, dest_encoder, carr_indexer, carr_encoder, origin_indexer, origin_encoder, vec_assembler])"
      ],
      "metadata": {
        "id": "fIgS4sFJgctR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "piped_data = flights_pipe.fit(df3).transform(df3)"
      ],
      "metadata": {
        "id": "ZnUiD-KPgki5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training, test = piped_data.randomSplit([.7, .3])"
      ],
      "metadata": {
        "id": "o-Y_UOdkgmNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression()"
      ],
      "metadata": {
        "id": "gYFmMa9lgnxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = lr.fit(training)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrpebJ88gqHb",
        "outputId": "288b3d77-72c9-49e6-9bde-070e53782361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegressionModel: uid=LogisticRegression_39a26109a26c, numClasses=2, numFeatures=700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "test_results = model.transform(test)\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "\n",
        "print(evaluator.evaluate(test_results))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwIsdlYNgsT3",
        "outputId": "ff0e537b-3fd9-4d34-8c3e-045b66279486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6666403192409428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def evaluate_model(training_data, test_data, features_to_drop=None):\n",
        "    \"\"\"Evaluates a logistic regression model with specified features dropped.\n",
        "\n",
        "    Args:\n",
        "        training_data: Spark DataFrame for training.\n",
        "        test_data: Spark DataFrame for testing.\n",
        "        features_to_drop: List of feature names to exclude.\n",
        "\n",
        "    Returns:\n",
        "        The area under ROC score of the model.\n",
        "    \"\"\"\n",
        "\n",
        "    input_cols = [\"DATE\", \"DISTANCE\", \"carrier_fact\", \"dest_fact\", \"origin_fact\"]\n",
        "    if features_to_drop:\n",
        "      input_cols = [col for col in input_cols if col.split(\"_\")[0] not in features_to_drop]\n",
        "\n",
        "    vec_assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
        "\n",
        "    flights_pipe = Pipeline(stages=[vec_assembler])\n",
        "\n",
        "    training_data = training_data.drop(\"features\")\n",
        "    piped_data = flights_pipe.fit(training_data).transform(training_data)\n",
        "\n",
        "    training, test = piped_data.randomSplit([.7, .3])\n",
        "\n",
        "    lr = LogisticRegression()\n",
        "    model = lr.fit(training)\n",
        "\n",
        "    test_results = model.transform(test)\n",
        "    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "    return evaluator.evaluate(test_results)\n",
        "\n",
        "\n",
        "# Original features\n",
        "original_score = evaluate_model(training, test)\n",
        "print(f\"Original model score: {original_score}\")\n",
        "\n",
        "# Without origin\n",
        "no_origin_score = evaluate_model(training, test, features_to_drop=[\"origin\"])\n",
        "print(f\"Model without origin score: {no_origin_score}\")\n",
        "\n",
        "# Without destination\n",
        "no_dest_score = evaluate_model(training, test, features_to_drop=[\"dest\"])\n",
        "print(f\"Model without destination score: {no_dest_score}\")\n",
        "\n",
        "# Without carrier\n",
        "no_carrier_score = evaluate_model(training, test, features_to_drop=[\"carrier\"])\n",
        "print(f\"Model without carrier score: {no_carrier_score}\")\n",
        "\n",
        "# Without month\n",
        "no_month_score = evaluate_model(training, test, features_to_drop=[\"DATE\"])\n",
        "print(f\"Model without month score: {no_month_score}\")\n"
      ],
      "metadata": {
        "id": "nAn1aHaPgvVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceb4b60d-1b16-4290-b9bb-8a6bdbcc1d33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original model score: 0.6653773777312674\n",
            "Model without origin score: 0.6471526953019525\n",
            "Model without destination score: 0.6428792647575441\n",
            "Model without carrier score: 0.655607860165325\n",
            "Model without month score: 0.6395241106222532\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}